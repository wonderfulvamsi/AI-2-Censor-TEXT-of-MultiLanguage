{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "#import nltk\n",
    "#from nltk import stopwords\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "#import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Labelz=preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25], dtype=int32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x='abcdefghijklmnopqrstuvwxyz'\n",
    "x=list(x)\n",
    "Labelz.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['f', 'u', 'n'], dtype='<U1')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Labelz.inverse_transform([5,20,13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(w):\n",
    "    w=list(w)\n",
    "    for i in range(len(w)):\n",
    "        #upper2lower case..\n",
    "        w[i]=w[i].lower()\n",
    "        #Removes Special charecters & numbers...\n",
    "        #w[i]=re.sub('[^ a-zA-Z]', '', w[i])\n",
    "        #PADDING\n",
    "    c=20-len(w)\n",
    "    w=list(Labelz.transform(w))\n",
    "    for i in range(c):\n",
    "        w.append(26)\n",
    "    return np.array(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "191\n",
      "17\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Common=['dick','bitch','motherfucker','sexy','fuck','ass','boobs','pussy','nudes','jackass','dumdass','dumbshit','shit','asshole']\n",
    "x=['niyema','koja','lanja','puku','puka','moda','sola','dengey','kojalanja','lanjakodaka','neyema','denga','neyeba','lanjapuka','boku','yedava','denginchuko','dengaa','bavarslanja','lanjaakodaka','mingey','badkoo','lawada','gudha','geru','boka','areepuka','jadepuka','modagudu','lapaki','sollu','kondapuka','pichilanja','pichipuka']\n",
    "z='zuze kosam paapam sona days siraj kaam bekhayali bulleya enna soda ban jevan rati rapuney banu ayaa chanaa mereya achaa chelta baby pull parle waste hulk avengers ultron savage people earth universe belong closer back seat rover know afford manage science technology silicon velipoodone facebook whatsapp solar power plant system indian china america pie die bye grain mail sleep movie idiot stupid brain artificial intelligence machine learning deep blue sea whale gain spain tree fifthy five fine brave being tweet cute google good bad better king spartha prince prize blood gone god worse enchanted hero pee busy gay kid green yellow animal place thing bee near far super man women chaduvu exams repu inka pelli prema kiss gud  raanu ravaa bonchesavaa intlo road bayata sweety sugar bear tiger wool meat neat hot temp car bike home phone call msg text chepuu ivu bandaa potii chitie darling love  hamayaa ostavaa koni evaru elaaunav gurtunanaa endukuu scientist community class implements regularized logistic regression using support only specify norm multiclass problems unavailable multinomial loss fit across  entire probability distribution lunar mission moon closest cosmic body space discovery attempted documented fast convergence guaranteed features approximately'\n",
    "h='Madarchod Bhenchod Lawde Gaandu Chutiya Randi chinaal Bhosadika Ludakeybal Chaka Madarjaat Haraami Randwa Bhosda Hijade Haramkhor chuth'\n",
    "z=z.split()\n",
    "h=h.split()\n",
    "\n",
    "print(len(x))\n",
    "print(len(z))\n",
    "print(len(h))\n",
    "\n",
    "#Labeling the Data..\n",
    "\n",
    "y1=[1 for i in range(48)]\n",
    "y2=[0 for i in range(191)]\n",
    "y3=[2 for i in range(17)]\n",
    "y=y1+y2+y3\n",
    "x=x+z+h+Common\n",
    "\n",
    "len(y)==len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(x)):\n",
    "    x[i]=clean(x[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model0=tree.DecisionTreeClassifier()\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model1=sklearn.ensemble.RandomForestClassifier()\n",
    "from sklearn.svm import SVC\n",
    "model2=sklearn.svm.SVC(gamma='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\vamsi\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "print(model0.fit(x,y))\n",
    "print(model1.fit(x,y))\n",
    "print(model2.fit(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(k):\n",
    "    p=model0.predict(k)\n",
    "    q=model1.predict(k)\n",
    "    r=model2.predict(k)\n",
    "\n",
    "    if p==q:\n",
    "        return r\n",
    "    if q==r:\n",
    "        return q\n",
    "    if p==r:\n",
    "        return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the Text u wanna Censor..\n",
      "hi ra puka\n",
      "['hi', 'ra', '****']\n"
     ]
    }
   ],
   "source": [
    "String=input(\"Enter the Text u wanna Censor..\\n\")\n",
    "o=String.split()\n",
    "\n",
    "#StopWordz=stopwords.words('english')\n",
    "\n",
    "LocalStopWordz=['clg','cls','arey','rey','bey','orey','anaa','mava','mama','papa','mayaa','tinnava','wassup','haa','haan','kyaa','kyu','kiku','nee','naa','ninu','nanu','ehe','lee','elee','paduko','wat','hehe','hahaa','haha','enti','ohh','ohkk','kkk','vamoo','vaamoo','enduku','ekada','unava','ippudu','pakka','ayaa','amma','nanaa','unnanu','ayoo','alaa','telusu','uffoo','uff','aaa','ahh','aboo','kuda','kani','maku','meku','sai','pic','pix','inkenti','maree','mari','kalisi','cinema','podam','bangaram','thali','bujii','babu','bhiya']\n",
    "\n",
    "\n",
    "for i in range(len(o)):\n",
    "    if len(o[i])>2:\n",
    "        if o[i] in LocalStopWordz:\n",
    "            pass\n",
    "        else:\n",
    "            k=o[i].lower()\n",
    "            k=clean(k)\n",
    "            k=np.array(k)\n",
    "            k=k.reshape(1,-1)\n",
    "            if predict(k)==1 or k in Common :\n",
    "                o[i]='****'\n",
    "            if predict(k)==2:\n",
    "                o[i]='####'\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note Still Lot of words need to be added to the z list..!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
